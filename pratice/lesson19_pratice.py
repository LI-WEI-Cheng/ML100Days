# RezNet
# 經網路結構都不會太深，主要是由於梯度消失(vanishing gradient)的問題。
# 下圖為經典的殘差結構，將輸入的input與經過2–3層的F(x)跨接並相加，使輸出表示為y=F(x)+x，
# 這樣的好處在於反向傳播時能保證至少會有一個1存在，降低梯度消失 (vanishing gradient) 發生的可能性。
# 沒有殘差網路的幫助下，模型結構越深反而造成準度下降。
# Residual Block :我們在做Back Propagation時，如果要求x的梯度，我們必須利用輸出y對x偏微分，當網路很深時容易造成梯度消失，
# 但如果鍊鎖率中每一個環節都保有一項x，自己對自己微分後都保有一個1，梯度消失的可能性就大幅降低，因此可以搭建更深的網路。

# 實現其實相當簡單，我們只需要將Block輸出加上輸入的值即可。
# 要注意的是輸入與輸出Feature Map必須有相同大小跟深度，不然無法直接相加。
# --> layers.add([x,input_tensor])




