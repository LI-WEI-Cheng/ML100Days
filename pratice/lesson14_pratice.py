#  Batch Normalization
# ------------------------------------------------------------------------------------------------------------------- #
# What is BN
# Normalization是數據標準化（歸一化，規範化），Batch可以理解為批量，加起來就是批量標準化。
# 先說Batch是怎麼確定的。在CNN中，Batch就是訓練網絡所設定的圖片數量batch_size。
# --------------------------------------------------------------------------- #
# Why is BN
# 解決的問題是梯度消失與梯度爆炸。
# 關於梯度消失，以sigmoid函數為例子，sigmoid函數使得輸出在[0,1]之間。
# 事實上x到了一定大小，經過sigmoid函數的輸出範圍就很小了
# 如果輸入很大，其對應的斜率就很小，我們知道，其斜率（梯度）在反向傳播中是權值學習速率。所以就會出現
# 在深度網絡中，如果網絡的激活輸出很大，其梯度就很小，學習速率就很慢。假設每層學習梯度都小於最大值0.25，網絡有n層，
# 因為鍊式求導的原因，第一層的梯度小於0.25的n次方，所以學習速率就慢，對於最後一層只需對自身求導1次，梯度就大，學習速率就快。
# 這會造成的影響是在一個很大的深度網絡中，淺層基本不學習，權值變化小，後面幾層一直在學習，結果就是，
# 後面幾層基本可以表示整個網絡，失去了深度的意義。
# ---------------------------------------------------------------------------
# How to use BN
# 卷積神經網絡CNN中5x5的圖片通過卷積得到的3x3特徵圖。
# 這裡假設通道數為1，batch為4，即大小為[4,1,3,3] (n,c,h,w)。
# 特徵圖裡的值，作為BN的輸入，這裡簡化輸出只有一個channel，也就是這一個4x3x3個數值通過BN計算並保存均值與方差，
# 並通過當前均值與方差計算歸一化的值，最後根據γ ,β以及歸一化得值計算BN層輸出。
# ------------------------------------------------------------------------------------------------------------------- #
# 資料分佈
# 一般來說我們都是以 Mini Batch的方式訓練資料，然而每一個 Batch 間的資料分佈可能不太相同，
# 而輸入每一層神經元的資訊分布也都可能會改變，造成收斂上的困難。
# 透過 BN，將每一層輸入資料的分佈歸一化為平均值為0，方差為1，確保資料分佈的穩定性。
# 然而 Normalize 改變資料的分佈，可能會造成上一層學到的特徵消失，
# 因此BN 的最後一步透過學習 Beta、Gamma，去微調 Normalize 後資料的分佈 。
# ------------------------------------------------------------------------------------------------------------------- #
# 梯度消失
# Sigmoid會將數值較大與較小的值通通壓在一起，並且由於其導函數最大值為0.25，容易發生梯度消失的情形，
# 透過BN，我們將資料分布歸一化，能有效降低梯度消失的可能性。
# ------------------------------------------------------------------------------------------------------------------- #
# normalization就是為了activation能更有效地使用輸入信息
# 近期論文都是先加Activation再加BN
from keras.models import Sequential
from keras.layers import Convolution2D
from keras.layers import MaxPooling2D
from keras.layers import Activation
from keras.layers import BatchNormalization
# ------------------------------------------------------------------------------------------------------------------- #
import os   # 去除bug
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
# ------------------------------------------------------------------------------------------------------------------- #
classifier=Sequential()
input = (32, 32, 3)
classifier.add(Convolution2D(32, kernel_size=(3, 3), padding='same',input_shape=input))
classifier.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))
classifier.add(BatchNormalization())
classifier.add(Activation('relu'))
classifier.summary()
# ------------------------------------------------------------------------------------------------------------------- #
# ------------------------------------------------------------------------------------------------------------------- #
print('---------------------------------------------------------------------------------------------------------------')
# 作業解答
input_shape = (32, 32, 3)
model = Sequential()
model.add(Convolution2D(32, kernel_size=(3, 3), padding='same', input_shape=input_shape))
model.add(BatchNormalization(momentum=0.99, epsilon=0.001))
model.add(Activation('sigmoid'))
model.add(Convolution2D(32, kernel_size=(3, 3), padding='same', input_shape=input_shape))
model.add(BatchNormalization(momentum=0.99, epsilon=0.001))
model.add(Activation('relu'))
model.summary()





























